<FrameworkSwitchCourse {fw} />

# Tokenizers（トークナイザ）

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_tf.ipynb"},
]} />

{/if}

<Youtube id="VFp38yj8h3A"/>

Tokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we'll explore exactly what happens in the tokenization pipeline. 
トークナイザはNLPパイプラインの重要な構成要素の1つです。トークナイザの目的は1つで、テキストをモデルが処理できるデータに変換することです。モデルが処理できるのは数値のみなので、トークナイザはテキストの入力を数値データに変換する必要があります。このセクションでは、トークン化パイプラインで何が起きているのかを具体的に説明します。

In NLP tasks, the data that is generally processed is raw text. Here's an example of such text:
NLPのタスクでは、一般的に処理されるデータは生文です。以下はその例です。

```
Jim Henson was a puppeteer （Jim Hensonは人形師でした）
```

However, models can only process numbers, so we need to find a way to convert the raw text to numbers. That's what the tokenizers do, and there are a lot of ways to go about this. The goal is to find the most meaningful representation — that is, the one that makes the most sense to the model — and, if possible, the smallest representation.
しかしながらモデルが処理できるのは数値のみなので、生文を数値に変換する方法を見つける必要があります。トークナイザはまさにこの役割を担っているものであり、変換にはさまざまな方法があります。目的はモデルにとって最も意味のある表現を見つけることです。そして可能であれば、最小な表現を見つけることも目的としています。

Let's take a look at some examples of tokenization algorithms, and try to answer some of the questions you may have about tokenization.
ここではトークン化アルゴリズムの例をいくつか見ながら、トークン化に関する疑問を解消していきます。

## 単語ベース

<Youtube id="nhJxYji1aho"/>

The first type of tokenizer that comes to mind is _word-based_. It's generally very easy to set up and use with only a few rules, and it often yields decent results. For example, in the image below, the goal is to split the raw text into words and find a numerical representation for each of them:
最初に思い浮かぶトークナイズ方法は、_単語ベース_ です。一般に、いくつかのルールを設定するだけで非常に簡単に使用でき、そして多くの場合において適切な結果を得ることができます。例えば以下の画像では、生のテキストを単語に分割し、それぞれの数値表現を見つけることが目的です。

<div class="flex justify-center">
  <img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg" alt="An example of word-based tokenization."/>
  <img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization-dark.svg" alt="An example of word-based tokenization."/>
</div>

There are different ways to split the text. For example, we could could use whitespace to tokenize the text into words by applying Python's `split()` function:
テキストの分け方にはさまざまな種類があります。例えば、Pythonの `split()` 関数を適用して、テキストを空白区切ることで単語に分割することができます。

```py
tokenized_text = "Jim Henson was a puppeteer".split()
print(tokenized_text)
```

```python out
['Jim', 'Henson', 'was', 'a', 'puppeteer']
```

There are also variations of word tokenizers that have extra rules for punctuation. With this kind of tokenizer, we can end up with some pretty large "vocabularies," where a vocabulary is defined by the total number of independent tokens that we have in our corpus.
また、単語トークン化には、句読点に関する特別なルールを持つものもあります。この種のトークナイザを使用すると、かなり大きな「語彙」を作成することができます。語彙は、コーパスに含まれるトークンの総数で定義されます。

Each word gets assigned an ID, starting from 0 and going up to the size of the vocabulary. The model uses these IDs to identify each word.
各単語には個別のID（0〜語彙のサイズ）が割り当てられます。モデルはこれらのIDを使用して各単語を識別します。

If we want to completely cover a language with a word-based tokenizer, we'll need to have an identifier for each word in the language, which will generate a huge amount of tokens. For example, there are over 500,000 words in the English language, so to build a map from each word to an input ID we'd need to keep track of that many IDs. Furthermore, words like "dog" are represented differently from words like "dogs", and the model will initially have no way of knowing that "dog" and "dogs" are similar: it will identify the two words as unrelated. The same applies to other similar words, like "run" and "running", which the model will not see as being similar initially.
単語ベースのトークナイザで言語を完全にカバーしようとすると、その言語の各単語に対応する識別子（ID）が必要になり、膨大な量のトークンが生成されることになります。例えば、英語には50万語以上の単語があるので、各単語から入力IDへのマップ（対応表）を作るには、それだけのIDを記録しておく必要があります。また、「dog」のような単語と「dogs」のような単語は表現が異なるため、モデルは最初 "dog" と "dogs" が似ていることを知ることができず、無関係な単語として認識してしまいます。また、"run" と "running" のような類似した単語についても同様で、モデルは初期状態では類似しているとは認識できません。

Finally, we need a custom token to represent words that are not in our vocabulary. This is known as the "unknown" token, often represented as "[UNK]" or "&lt;unk&gt;". It's generally a bad sign if you see that the tokenizer is producing a lot of these tokens, as it wasn't able to retrieve a sensible representation of a word and you're losing information along the way. The goal when crafting the vocabulary is to do it in such a way that the tokenizer tokenizes as few words as possible into the unknown token.
最後に、語彙にない単語 （未知語）を表すためのトークンが必要です。これは "unknown" トークンと呼ばれ、"[UNK]" や "&lt;unk&gt;" として表されます。トークナイザが多くの unknown トークンを生成している場合、単語の適切な表現を取得できず、情報が失われていると解釈できます。語彙を作成する際の目標は、unknownトークンにトークン化されてしまう単語（未知語）がより少なくなるようにすることです。

One way to reduce the amount of unknown tokens is to go one level deeper, using a _character-based_ tokenizer.
unknown トークンの総数を減らす方法の1つは、_文字ベース_ のトークナイザを使用することです。

## 文字ベース

<Youtube id="ssLq_EK2jLE"/>

Character-based tokenizers split the text into characters, rather than words. This has two primary benefits:
_文字ベース_ トークナイザはテキストを単語単位ではなく文字単位で分割します。これには2つの主な利点があります。

- The vocabulary is much smaller.
- There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters.

- 語彙サイズがはるかに小さくなります。
- すべての単語は文字で構成されるため、語彙外の (unknown) トークンがはるかに少なくなります。

But here too some questions arise concerning spaces and punctuation:
しかし、ここでも空白と句読点に関する問題が発生します。

<div class="flex justify-center">
  <img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization.svg" alt="An example of character-based tokenization."/>
  <img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization-dark.svg" alt="An example of character-based tokenization."/>
</div>

This approach isn't perfect either. Since the representation is now based on characters rather than words, one could argue that, intuitively, it's less meaningful: each character doesn't mean a lot on its own, whereas that is the case with words. However, this again differs according to the language; in Chinese, for example, each character carries more information than a character in a Latin language.
このアプローチも先と同様、完璧なものではありません。ここでは、表現が単語ではなく文字に基づいているので、直感的にはテキストの意味をうまく汲み取れないとも考えられます。各文字は単独ではあまり意味を持たないのに対し、単語はそのようなことはありません。しかし、言語によってはここでも違いがあります。例えば中国語の各文字は、ラテン語の文字よりも情報を持っています。（漢字1文字とアルファベット1文字では、表現している情報量が異なる場合がありますね。）

Another thing to consider is that we'll end up with a very large amount of tokens to be processed by our model: whereas a word would only be a single token with a word-based tokenizer, it can easily turn into 10 or more tokens when converted into characters.
考慮すべきもう1つの点としては、モデルが処理する必要があるトークンの数が非常に多くなってしまうことです。単語ベースのトークナイザでは、単語は1つのトークンになりますが、文字ベースのトークナイザでは、単語は10個以上のトークンに変換される可能性があります。

To get the best of both worlds, we can use a third technique that combines the two approaches: *subword tokenization*.
両者のいいとこ取りをするために、これらのアプローチを組み合わせた第3の手法を使用することができます。それが *サブワードトークン化* です。

## サブワードトークン化

<Youtube id="zHvTiHr506c"/>

Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords.
サーブワードトークン化アルゴリズムは、「出現頻度の高い単語は小さなサブワードに分割されるべきではないが、出現頻度の低い単語は意味のあるサブワードに分割されるべきである」という原理に基づいています。

For instance, "annoyingly" might be considered a rare word and could be decomposed into "annoying" and "ly". These are both likely to appear more frequently as standalone subwords, while at the same time the meaning of "annoyingly" is kept by the composite meaning of "annoying" and "ly".
例えば "annoyingly" は出現頻度の低い単語として扱われ、"annoying" と "ly" に分割されることがあります。これら2つのサブワードは、それぞれ単独で頻繁に出現する可能性がありますが、一方で "annoyingly" は稀な単語なので、その意味を "annoying" と "ly" の合成語として表現しようという考え方になります。

Here is an example showing how a subword tokenization algorithm would tokenize the sequence "Let's do tokenization!":
それではここで、サブワードトークン化アルゴリズムが "Let's do tokenization!" という系列をトークン化する様子を見てみましょう。

<div class="flex justify-center">
  <img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg" alt="A subword tokenization algorithm."/>
  <img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword-dark.svg" alt="A subword tokenization algorithm."/>
</div>

These subwords end up providing a lot of semantic meaning: for instance, in the example above "tokenization" was split into "token" and "ization", two tokens that have a semantic meaning while being space-efficient (only two tokens are needed to represent a long word). This allows us to have relatively good coverage with small vocabularies, and close to no unknown tokens.
これらのサブワードは最終的に、うまく意味を表現したものとして機能します。例えば上の例では "tokenization" は "token" と "ization" に分割されていましたが、これら2つのトークンは、空間効率が良く（2つのトークンだけで長い単語を表現できている）、意味論的にも有意なものとなっています。これにより、比較的小さな語彙で多くの単語をカバーすることができ、未知語がほとんど出ないようになります。

This approach is especially useful in agglutinative languages such as Turkish, where you can form (almost) arbitrarily long complex words by stringing together subwords.
このアプローチはトルコ語などの膠着語（機能語が自立語にくっついて文が構成される言語）において特に有効です。トルコ語では、サブワードを繋げることで（ほぼ）任意の長さの合成語を作ることができます。

### さらなるトークン化手法！

Unsurprisingly, there are many more techniques out there. To name a few:
実は他にも多くのトークン化手法が存在し、例えば以下のようなものがあります。

- Byte-level BPE, as used in GPT-2
- WordPiece, as used in BERT
- SentencePiece or Unigram, as used in several multilingual models

- Byte-level BPE: GPT-2で使用される手法
- WordPiece: BERTで使用される手法
- SentencePiece もしくは Unigram: いくつかの多言語モデルで使用される手法

You should now have sufficient knowledge of how tokenizers work to get started with the API.
ここまで読めば、APIを使ってトークナイザを使い始めるために必要な知識は十分に身についていると思います！

## 読み込みと保存

Loading and saving tokenizers is as simple as it is with models. Actually, it's based on the same two methods: `from_pretrained()` and `save_pretrained()`. These methods will load or save the algorithm used by the tokenizer (a bit like the *architecture* of the model) as well as its vocabulary (a bit like the *weights* of the model).
トークナイザの読み込みと保存は、モデルと同様に簡単です。実際、これらは同じ2つのメソッド `from_pretrained()` と `save_pretrained()` に基づいています。これらのメソッドは、トークナイザが使用するアルゴリズム（モデルでいう *アーキテクチャ*）と、語彙（モデルでいう *重み*）を読み込むか保存するかを決定します。

Loading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model, except we use the `BertTokenizer` class:
BERTとなじチェックポイントで学習されたBERTトークナイザを読み込む方法は、モデルでの読み込み方法と同じです。ただし、`BertTokenizer` クラスを使う点だけが異なります。

```py
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
```

{#if fw === 'pt'}
Similar to `AutoModel`, the `AutoTokenizer` class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint:
`AutoModel` と同様に、`AutoTokenizer` クラスはチェックポイント名に基づいてライブラリ内の適切なトークナイザクラスを取得し、任意のチェックポイントを直接使用することができます。

{:else}
Similar to `TFAutoModel`, the `AutoTokenizer` class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint:
`TFAutoModel` と同様に、`AutoTokenizer` クラスはチェックポイント名に基づいてライブラリ内の適切なトークナイザクラスを取得し、任意のチェックポイントを直接使用することができます。

{/if}

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

We can now use the tokenizer as shown in the previous section:
そして、前のセクションで見たようにトークナイザを使用することができます。

```python
tokenizer("Using a Transformer network is simple")
```

```python out
{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

Saving a tokenizer is identical to saving a model:
トークナイザの保存は、モデルの保存と同じ方法でできます。

```py
tokenizer.save_pretrained("directory_on_my_computer")
```

We'll talk more about `token_type_ids` in [Chapter 3](/course/chapter3), and we'll explain the `attention_mask` key a little later. First, let's see how the `input_ids` are generated. To do this, we'll need to look at the intermediate methods of the tokenizer.
`token_type_ids` については[Chapter3](/course/chapter3)で詳しく説明し、`attention_mask` についても後ほど説明します。まずは `input_ids` がどのように生成されるかを見てみましょう。

## エンコーディング

<Youtube id="Yffk5aydLzg"/>

Translating text to numbers is known as _encoding_. Encoding is done in a two-step process: the tokenization, followed by the conversion to input IDs.
テキストを数値に変換することを _エンコード_ と呼びます。エンコードはトークン化とその後の入力IDへの変換の2段階のプロセスで行われます。

As we've seen, the first step is to split the text into words (or parts of words, punctuation symbols, etc.), usually called *tokens*. There are multiple rules that can govern that process, which is why we need to instantiate the tokenizer using the name of the model, to make sure we use the same rules that were used when the model was pretrained.
ここまで見てきたように、最初のステップはテキストをトークン（単語や単語の一部、句読点など）に分割することです。このプロセスを管理するためには、いくつかのルールがあります。そのため、モデルの名前を使ってトークナイザをインスタンス化する必要があります。これにより、モデルが事前学習されたときに使用されたものと同じルールを使用することができます。

The second step is to convert those tokens into numbers, so we can build a tensor out of them and feed them to the model. To do this, the tokenizer has a *vocabulary*, which is the part we download when we instantiate it with the `from_pretrained()` method. Again, we need to use the same vocabulary used when the model was pretrained.
2番目のステップはトークンを数値に変換することです。これにより、テンソルを構築し、モデルに入力することができます。これを行うために、トークナイザは *語彙* を有しています。これは、`from_pretrained()` メソッドでインスタンス化するときにダウンロードされる部分です。繰り返しになりますが、モデルの事前学習で使用された語彙と同じものを使用する必要があることに注意してください。

To get a better understanding of the two steps, we'll explore them separately. Note that we will use some methods that perform parts of the tokenization pipeline separately to show you the intermediate results of those steps, but in practice, you should call the tokenizer directly on your inputs (as shown in the section 2).
この2つの理解を深めるために、それぞれのステップを別々に見ていきます。ステップの中間結果を表示するために、トークン化パイプラインの一部を別々に実行するメソッドを使用しますが、実際には（セクション2で見たように）入力に対して直接トークナイザを呼び出す必要があります。

### トークン化

The tokenization process is done by the `tokenize()` method of the tokenizer:
トークン化のプロセスは `tokenize()` メソッドによって行われます。

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)

print(tokens)
```

The output of this method is a list of strings, or tokens:
このメソッドの出力はトークンもしくは文字のリストです。

```python out
['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']
```

This tokenizer is a subword tokenizer: it splits the words until it obtains tokens that can be represented by its vocabulary. That's the case here with `transformer`, which is split into two tokens: `transform` and `##er`.
これはサブワードトークナイザとなっているため、語彙に含まれるトークンになるまで単語を分割します。ここでは `transformer` が `transform` と `##er` に分割されているのがわかります。

### トークンからIDへの変換

The conversion to input IDs is handled by the `convert_tokens_to_ids()` tokenizer method:
トークンからIDへの変換は `convert_tokens_to_ids()` トークナイザメソッドによって行われます。

```py
ids = tokenizer.convert_tokens_to_ids(tokens)

print(ids)
```

```python out
[7993, 170, 11303, 1200, 2443, 1110, 3014]
```

These outputs, once converted to the appropriate framework tensor, can then be used as inputs to a model as seen earlier in this chapter.
これらの出力は、適切なフレームワークのテンソルに変換された後、前述のようにモデルの入力として使用できます。

<Tip>

✏️ **Try it out!** Replicate the two last steps (tokenization and conversion to input IDs) on the input sentences we used in section 2 ("I've been waiting for a HuggingFace course my whole life." and "I hate this so much!"). Check that you get the same input IDs we got earlier!
✏️ **試してみよう！** 最後の2つのステップ（トークン化と入力IDへの変換）を、セクション2で使った入力文（"I've been waiting for a HuggingFace course my whole life." と "I hate this so much!"）に対して再現してみましょう。先ほどと同じ入力IDが得られるかどうかを確認してみてください。

</Tip>

## デコーディング

*Decoding* is going the other way around: from vocabulary indices, we want to get a string. This can be done with the `decode()` method as follows:
*デコーディング* はエンコーディングとは逆の処理になります。`decode()` メソッドを使うことで、語彙のインデックスから文字列を取得することができます。

```py
decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)
```

```python out
'Using a Transformer network is simple'
```

Note that the `decode` method not only converts the indices back to tokens, but also groups together the tokens that were part of the same words to produce a readable sentence. This behavior will be extremely useful when we use models that predict new text (either text generated from a prompt, or for sequence-to-sequence problems like translation or summarization).
`decode` メソッドは語彙のインデックスをトークンに戻すだけでなく、同じ単語の一部であったトークンをまとめて、読みやすい文章に変換してくれることに注意してください。この挙動は、プロンプトから生成されたテキストや、翻訳や要約などの系列から系列への変換などの問題を解くモデルを使うときに非常に役に立ちます。

By now you should understand the atomic operations a tokenizer can handle: tokenization, conversion to IDs, and converting IDs back to a string. However, we've just scraped the tip of the iceberg. In the following section, we'll take our approach to its limits and take a look at how to overcome them.
ここまでで、トークナイザでできる基本的な処理（トークン化、IDへの変換、IDから文字列への変換）を理解できたのではないでしょうか。しかし、これは氷山の一角にすぎません。次のセクションでは、これらの処理を限界まで拡張していき、その限界を超える方法を見ていきましょう。
